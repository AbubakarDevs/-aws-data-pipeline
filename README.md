# ğŸš€ AWS Data Pipeline â€“ ETL with PySpark, Glue & S3

### ğŸ§© Overview
This project demonstrates a scalable **ETL pipeline** built on **AWS** using **S3, Glue, PySpark, and Athena**.  
It simulates a real-world use case of processing raw CSV data into analytics-ready tables.

---

### ğŸ—ï¸ Architecture
- **Data Source:** Sample CSV files (customer transactions)
- **Storage:** AWS S3 (raw â†’ curated layers)
- **Processing:** AWS Glue (PySpark jobs)
- **Querying:** AWS Athena
- **Orchestration:** AWS Step Functions

---

### âš™ï¸ Workflow
1. Raw data ingested to `s3://data-pipeline/raw/`
2. Glue ETL job cleans & transforms using PySpark
3. Output saved to `s3://data-pipeline/curated/`
4. Athena used for querying processed data

---

### ğŸ“Š Technologies
AWS S3 Â· AWS Glue Â· PySpark Â· Athena Â· Step Functions Â· IAM Â· CloudWatch

---

### ğŸ“ˆ Key Learnings
- Building cost-efficient ETL pipelines on AWS
- Handling schema evolution using Glue Catalog
- Implementing data quality checks and logs

